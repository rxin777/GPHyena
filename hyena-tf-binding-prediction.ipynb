{"cells":[{"cell_type":"markdown","metadata":{"id":"71JrV2XfoBem"},"source":["# GPH - Genomic Pretrained Model with Hyena\n","\n","**Author:** Riley Xin\n","\n","**Date:** May 18, 2025\n","\n","**Class:** GENE46100\n","\n","---\n","\n","This notebook implements a **Hyena-based genomic language model**, adapted from:\n","\n","- [The Annotated Hyena Blog Post](https://medium.com/autonomous-agents/evo2-demystified-the-ultimate-technical-guide-to-genomic-language-modeling-a75b0afe7b87)  \n","- [The Annotated Hyena Notebook](https://github.com/expz/annotated-hyena/blob/master/annotated_hyena.ipynb)  \n","- Henry’s nanoGPT notebook\n","- Andrej Karpathy's [nanoGPT framework](https://github.com/karpathy/nanoGPT)\n","\n","The code is configured to run on Google Colab with T4 GPU. It includes an implementation of the Hyena operator and a transformer-style backbone similar to nano-GPT, where the self-attention block can be directly replaced by a Hyena block. The model is first pretrained on human genome FASTA data, followed by a fine-tuning example on an enhancer classification task.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ELE7Jqh1vOGN"},"outputs":[],"source":["#@title Setup\n","%%capture\n","# !pip install torch numpy datasets lightning huggingface_hub regex tiktoken wandb tqdm accelerate\n","import os\n","import pickle\n","import requests\n","import numpy as np\n","import pandas as pd\n","import math\n","import random\n","import inspect\n","import regex\n","# import lightning\n","from typing import Any, Dict, List, Optional, Tuple, Type\n","from dataclasses import dataclass, asdict\n","import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","import gc\n","import time\n","from contextlib import nullcontext\n","from torch.nn.parallel import DistributedDataParallel as DDP\n","from datasets import load_dataset, Dataset\n","from torch.utils.data import DataLoader, TensorDataset\n","from sklearn.metrics import matthews_corrcoef, f1_score\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","source":["# Some issues related to load_dataset having to do with package versions on colab\n","# !pip install -U datasets"],"metadata":{"id":"59a6E4lB4f-T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Mount to drive and prepare the genome data\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","DIR = \"/content/drive/MyDrive/UChicago/Spring2025/GENE46100/project\"\n","\n","data_dir = os.path.join(DIR, \"data\")\n","fasta_file = os.path.join(data_dir, \"genome.fa\")\n","!wget -O - https://hgdownload.soe.ucsc.edu/goldenPath/hg38/bigZips/hg38.fa.gz | gunzip -c > {fasta_file}"],"metadata":{"id":"i5wfeEBCrP2q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fasta_file = os.path.join(data_dir, \"genome.fa\")\n","with open(fasta_file, 'r') as f:\n","    data = f.read()\n","print(f\"length of dataset in characters: {len(data):,}\")\n","\n","chars = sorted(list(set(data)))\n","vocab_size = len(chars)\n","print(\"all the unique characters:\", ''.join(chars))\n","print(f\"vocab size: {vocab_size:,}\")\n","data[:1]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":104},"id":"2F6FImUcMLmU","executionInfo":{"status":"ok","timestamp":1747504075790,"user_tz":300,"elapsed":58991,"user":{"displayName":"Riley Xin","userId":"01480201500918314054"}},"outputId":"49141532-af52-4081-d695-9e5d5d4c5cf3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["length of dataset in characters: 3,273,481,150\n","all the unique characters: \n","0123456789>ABCGHIJKLMNTUXY_acdghlmnortv\n","vocab size: 40\n"]},{"output_type":"execute_result","data":{"text/plain":["'>'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":8}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E9-U2ndwSfbg"},"outputs":[],"source":["#@title Tokenization\n","# create a mapping from characters to integers\n","stoi = { ch:i for i,ch in enumerate(chars) }\n","itos = { i:ch for i,ch in enumerate(chars) }\n","\n","def encode(s):\n","    return [stoi[c] for c in s] # encoder: take a string, output a list of integers\n","def decode(l):\n","    return ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n","\n","s = \"ACGTACGT\"\n","print(encode(s))\n","print(decode(encode(s)))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7ZeoG40W0Do1","outputId":"e63f36c8-4b8a-48d5-afe0-a8ebf7734be4","executionInfo":{"status":"ok","timestamp":1747502229248,"user_tz":300,"elapsed":575322,"user":{"displayName":"Riley Xin","userId":"01480201500918314054"}},"collapsed":true},"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenizing: 0% complete\n","Tokenizing: 5% complete\n","Tokenizing: 10% complete\n","Tokenizing: 15% complete\n","Tokenizing: 20% complete\n","Tokenizing: 25% complete\n","Tokenizing: 30% complete\n","Tokenizing: 35% complete\n","Tokenizing: 40% complete\n","Tokenizing: 45% complete\n","Tokenizing: 50% complete\n","Tokenizing: 55% complete\n","Tokenizing: 60% complete\n","Tokenizing: 65% complete\n","Tokenizing: 70% complete\n","Tokenizing: 75% complete\n","Tokenizing: 80% complete\n","Tokenizing: 85% complete\n","Tokenizing: 90% complete\n","Tokenizing: 95% complete\n","train has 2,946,133,035 tokens\n","val has 327,348,115 tokens\n"]}],"source":["# Create the train and test splits\n","n = len(data)\n","train_data = data[:int(n*0.9)]\n","val_data = data[int(n*0.9):]\n","\n","# Helper function to store/append data to binary files and save RAM\n","def append_to_binary_file(filename, obj):\n","    with open(filename, 'ab') as file:  # Open in append-binary mode\n","        obj.tofile(file)\n","\n","# Encode both to integers\n","ntrain = len(train_data)\n","nval = len(val_data)\n","for i in np.arange(0, 1, 0.05):\n","    print(f\"Tokenizing: {int(i*100):,}% complete\")\n","    train_ids = encode(train_data[int(ntrain*i):int(ntrain*(i+0.05))])\n","    val_ids = encode(val_data[int(nval*i):int(nval*(i+0.05))])\n","    train_ids = np.array(train_ids, dtype=np.uint16)\n","    val_ids = np.array(val_ids, dtype=np.uint16)\n","    append_to_binary_file(os.path.join(data_dir, 'train.bin'), train_ids)\n","    append_to_binary_file(os.path.join(data_dir, 'val.bin'), val_ids)\n","\n","print(f\"train has {ntrain:,} tokens\")\n","print(f\"val has {nval:,} tokens\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DsBvXgTc0Xsw"},"outputs":[],"source":["# Save the meta information as well, to help us encode/decode later\n","meta = {\n","    'vocab_size': vocab_size,\n","    'itos': itos,\n","    'stoi': stoi,\n","}\n","with open(os.path.join(data_dir, 'meta.pkl'), 'wb') as f:\n","    pickle.dump(meta, f)"]},{"cell_type":"code","source":["#@title Load saved data\n","# Avoid rerunning the data processing steps\n","data_dir = os.path.join(DIR, \"data\")\n","\n","# Load the meta information (for encoding/decoding)\n","with open(os.path.join(data_dir, 'meta.pkl'), 'rb') as f:\n","    meta = pickle.load(f)\n","stoi = meta['stoi']\n","itos = meta['itos']\n","vocab_size = meta['vocab_size']\n","\n","# Define encode/decode functions\n","encode = lambda s: [stoi[c] for c in s]\n","decode = lambda l: ''.join([itos[i] for i in l])\n","\n","# Load the tokenized train/val data\n","train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n","val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n","\n","print(f\"Loaded train_data with {len(train_data):,} tokens\")\n","print(f\"Loaded val_data with {len(val_data):,} tokens\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HsH046c8SOwf","executionInfo":{"status":"ok","timestamp":1747614022057,"user_tz":300,"elapsed":1770,"user":{"displayName":"Riley Xin","userId":"01480201500918314054"}},"outputId":"d7ab1b5b-de90-49eb-8868-0039f9c0885b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded train_data with 2,946,133,035 tokens\n","Loaded val_data with 327,348,115 tokens\n"]}]},{"cell_type":"markdown","source":["# Creating the Model"],"metadata":{"id":"3WMZZleSOaD5"}},{"cell_type":"markdown","metadata":{"id":"jGwl1lk6K4JI"},"source":["Now that we’ve load our training data, we turn to constructing the full GP-Hyena model. The architecture is modeled after nanoGPT, with the standard attention mechanism replaced by the Hyena operator, which persumably enables more efficient long-range modeling. Below is an overview of the model’s structure and components.\n","\n","The main model is implemented in a class called GPH (Genomic Pretrained Model with Hyena). It consists of several core components:\n","\n","* **Embedding**: Genomic tokens (from a 40-character vocabulary) are projected into a higher-dimensional latent space using token and position embeddings. This allows the model to represent more complex patterns in sequence data, including positional context.\n","\n","* **LayerNorm**: These normalization layers stabilize training by maintaining feature-wise consistency in scale, applied before Hyena operations and MLPs.\n","\n","* **Dropout**: Applied both after embeddings and within blocks, dropout helps regularize training and prevent overfitting.\n","\n","* **Hyena Block**: The central part in GP-Hyena. Instead of using self-attention, this block uses a long convolutional operator (Hyena) to model sequence dependencies in a more scalable way. Each Hyena block includes a projection, filtering mechanism, and nonlinear transformation via FFT-based convolution.\n","\n","* **MLP**: A feedforward module consisting of a two-layer linear projection with a GELU activation in between. It adds non-linearity and capacity to the model.\n","\n","* **Block**: A full processing unit in the model, defined as: LayerNorm → Hyena Block → LayerNorm → MLP. These are stacked multiple times (e.g. 6 blocks for a 6-layer model).\n","\n","* **Final LayerNorm and Output Head**: After passing through all blocks, the output is normalized again and projected to the vocabulary space using a linear head. If training, loss is calculated using cross-entropy.\n","\n","The forward pass of GPH proceeds in the following order:\n","\n","1. Token and Position Embedding Layers\n","\n","2. Dropout Layer\n","\n","3. Sequential Block Stack (each containing Hyena + MLP)\n","\n","4. Final LayerNorm\n","\n","5. Linear Projection to Vocabulary\n","\n","6. Loss Computation (optional during training)\n","\n","We will show an implementation of each component followed by the training loop for training the GPH model.\n"]},{"cell_type":"code","source":["#@title Define the Hyena block and the GP model backbone\n","@dataclass(kw_only=True)\n","class Config:\n","    # Hyena-specific\n","    d_model: int # overall\n","    d_embed: int # Hyena filter\n","    d_filter_mlp: int\n","    n_filter_layers: int\n","    context_length: int\n","    short_conv_size: int\n","    order: int\n","    pdrop_hyena: float\n","    pdrop_embed: float\n","    omega: Optional[int]\n","    n_layers: int\n","    vocab_size: int\n","    bias: bool\n","    # Training-specific\n","    learning_rate: float\n","    epochs: int\n","    betas: Tuple[float, float]\n","    weight_decay: float\n","    device_type: str\n","    batch_size: int\n","\n","class Projection(nn.Module):\n","    def __init__(self, d_model: int, N: int, conv_len: int):\n","        super().__init__()\n","        self.d_model = d_model\n","        self.linear = nn.Linear(d_model, d_model * (N + 1))\n","        self.conv = nn.Conv1d(\n","            in_channels=d_model * (N + 1),\n","            out_channels=d_model * (N + 1),\n","            kernel_size=conv_len,\n","            groups=d_model * (N + 1),\n","            padding=conv_len - 1\n","        )\n","\n","    def forward(self, u):\n","        z = self.linear(u)\n","        z = z.transpose(1, 2)\n","        L = z.shape[-1]\n","        z = self.conv(z)[..., :L]\n","        return torch.split(z, self.d_model, dim=1)\n","\n","class FFTConv(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, h, x, B):\n","        L = h.shape[-1]\n","        f_h = torch.fft.rfft(h, n=2 * L, norm=\"forward\")\n","        f_x = torch.fft.rfft(x.to(dtype=h.dtype), n=2 * L)\n","        y = torch.fft.irfft(f_h * f_x, n=2 * L, norm=\"forward\")[..., :L]\n","        y = y + x * B\n","        return y.to(dtype=h.dtype)\n","\n","class Window(nn.Module):\n","    def __init__(\n","        self,\n","        d_model: int,\n","        max_seq_len: int,\n","        fast_decay_pct: float = 0.3,\n","        slow_decay_pct: float = 1.5,\n","        target: float = 1e-2,\n","    ):\n","        super().__init__()\n","        self.b = nn.Parameter(torch.zeros((1, d_model, 1)))\n","        min_decay = math.log(target) / slow_decay_pct\n","        max_decay = math.log(target) / fast_decay_pct\n","        self.alphas = nn.Parameter(\n","            torch.linspace(min_decay, max_decay, d_model)[None, :, None]\n","        )\n","        self.t = nn.Parameter(\n","            torch.linspace(0, 1, max_seq_len)[None, None, :],\n","            requires_grad=False\n","        )\n","\n","    def forward(self, x):\n","        L = x.shape[2]\n","        c = torch.exp(self.alphas * self.t)[:, :, :L]\n","        return x * (c + self.b)\n","\n","class HyenaFilter(nn.Module):\n","    def __init__(\n","        self,\n","        d_model: int,\n","        d_mlp: int,\n","        d_embed: int,\n","        N: int,\n","        n_layers: int = 4,\n","        max_seq_len: int = 128,\n","        omega: int = 8,\n","    ):\n","        assert n_layers >= 2, \"n_layers must be at least 2\"\n","        super().__init__()\n","\n","        self.N = N\n","        self.d_model = d_model\n","        self.h = nn.Parameter(torch.randn((N, d_model, max_seq_len)))\n","        self.window = Window(d_embed, max_seq_len)\n","\n","    def forward(self, L: int) -> torch.Tensor:\n","        h = self.h[:, :, :L]  # [N, d_embed, L]\n","        h = self.window(h)\n","        h = h / torch.norm(h, dim=-2, p=1, keepdim=True)\n","        return h\n","\n","class HyenaBlock(nn.Module):\n","    def __init__(self, config: Config):\n","        super().__init__()\n","        self.proj_input = Projection(config.d_model, config.order, config.short_conv_size)\n","        self.proj_output = nn.Linear(config.d_model, config.d_model)\n","        self.filter = HyenaFilter(\n","            config.d_model,\n","            config.d_filter_mlp,\n","            config.d_embed,\n","            config.order,\n","            config.n_filter_layers,\n","            config.context_length,\n","            config.omega,\n","        )\n","        self.dropout = nn.Dropout(config.pdrop_hyena)\n","        self.fft_conv = FFTConv()\n","        self.B = nn.Parameter(torch.randn((config.order, 1, config.d_model, 1)))\n","\n","    def forward(self, u: torch.Tensor) -> torch.Tensor:\n","        L = u.shape[1]\n","        *x, v = self.proj_input(u)\n","        v = v + u.transpose(1, 2)\n","        h = self.filter(L)\n","        for i, x_i in enumerate(x):\n","            h_i = h[i].unsqueeze(0)\n","            v = v + F.normalize(x_i, dim=1) * self.fft_conv(h_i, v, self.B[i])\n","        v = v.transpose(1, 2)\n","        y = v + self.dropout(self.proj_output(v))\n","        return y\n","\n","class LayerNorm(nn.Module):\n","    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n","\n","    def __init__(self, ndim, bias):\n","        super().__init__()\n","        self.weight = nn.Parameter(torch.ones(ndim))\n","        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n","\n","    def forward(self, input):\n","        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n","\n","\n","class MLP(nn.Module):\n","\n","    def __init__(self, config):\n","        super().__init__()\n","        self.c_fc    = nn.Linear(config.d_model, 4 * config.d_model, bias=config.bias)\n","        self.gelu    = nn.GELU()\n","        self.c_proj  = nn.Linear(4 * config.d_model, config.d_model, bias=config.bias)\n","        self.dropout = nn.Dropout(config.pdrop_embed)\n","\n","    def forward(self, x):\n","        x = self.c_fc(x)\n","        x = self.gelu(x)\n","        x = self.c_proj(x)\n","        x = self.dropout(x)\n","        return x\n","\n","class Block(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.ln_1 = LayerNorm(config.d_model, bias=config.bias)\n","        self.hyena = HyenaBlock(config)\n","        self.ln_2 = LayerNorm(config.d_model, bias=config.bias)\n","        self.mlp = MLP(config)\n","\n","    def forward(self, x):\n","        val = self.hyena(self.ln_1(x))\n","        x = x + val\n","        x = x + self.mlp(self.ln_2(x))\n","        return x\n","\n","class GPH(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        assert config.vocab_size is not None\n","        assert config.context_length is not None\n","        self.config = config\n","\n","        self.tok_emb = nn.Embedding(config.vocab_size, config.d_model)\n","        self.pos_emb = nn.Parameter(torch.randn(1, config.context_length, config.d_model))\n","        self.drop = nn.Dropout(config.pdrop_embed)\n","\n","        self.blocks = nn.Sequential(*[Block(config) for _ in range(config.n_layers)])\n","        self.ln_f = LayerNorm(config.d_model, bias=config.bias)\n","        self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n","        self.lm_head.weight = self.tok_emb.weight  # weight tying\n","\n","        self.apply(self._init_weights)\n","        for pn, p in self.named_parameters():\n","            if pn.endswith('c_proj.weight'):\n","                torch.nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * config.n_layers))\n","\n","        print(\"number of parameters: %.2fM\" % (self.get_num_params() / 1e6,))\n","\n","    def get_num_params(self, non_embedding=True):\n","        n_params = sum(p.numel() for p in self.parameters())\n","        return n_params\n","\n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n","            if module.bias is not None:\n","                torch.nn.init.zeros_(module.bias)\n","        elif isinstance(module, nn.Embedding):\n","            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n","\n","    def forward(self, idx, targets=None):\n","        b, t = idx.size()\n","        assert t <= self.config.context_length, f\"Cannot forward sequence of length {t}, context length is only {self.config.context_length}\"\n","\n","        x = self.tok_emb(idx) + self.pos_emb[:, :t, :]\n","        x = self.drop(x)\n","        x = self.blocks(x)\n","        x = self.ln_f(x)\n","        logits = self.lm_head(x)\n","\n","        loss = None\n","        if targets is not None:\n","            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n","\n","        return logits, loss\n","\n","    def configure_optimizers(self, weight_decay=None, learning_rate=None, betas=None, device_type=None):\n","        weight_decay = weight_decay or self.config.weight_decay\n","        learning_rate = learning_rate or self.config.learning_rate\n","        betas = betas or self.config.betas\n","        device_type = device_type or self.config.device_type\n","\n","        param_dict = {pn: p for pn, p in self.named_parameters() if p.requires_grad}\n","        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n","        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n","        optim_groups = [\n","            {'params': decay_params, 'weight_decay': weight_decay},\n","            {'params': nodecay_params, 'weight_decay': 0.0}\n","        ]\n","        print(f\"num decayed parameter tensors: {len(decay_params)}, with {sum(p.numel() for p in decay_params):,} parameters\")\n","        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {sum(p.numel() for p in nodecay_params):,} parameters\")\n","\n","        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n","        use_fused = fused_available and device_type == 'cuda'\n","        extra_args = dict(fused=True) if use_fused else dict()\n","\n","        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n","        print(f\"using fused AdamW: {use_fused}\")\n","        return optimizer\n","\n","    def estimate_mfu(self, fwdbwd_per_iter, dt):\n","        \"\"\" Estimate model flops utilization (MFU) in units of A100 bfloat16 peak FLOPS \"\"\"\n","        N = self.get_num_params()\n","        cfg = self.config\n","        L, H, Q, T = cfg.n_layers, 1, cfg.d_model, cfg.context_length  # Hyena uses H=1 (no heads)\n","        flops_per_token = 6*N + 12*L*H*Q*T\n","        flops_per_fwdbwd = flops_per_token * T\n","        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter\n","        flops_achieved = flops_per_iter * (1.0 / dt)\n","        flops_promised = 312e12  # A100 peak TFLOPs for bfloat16\n","        return flops_achieved / flops_promised\n","\n","    @torch.no_grad()\n","    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n","        for _ in range(max_new_tokens):\n","            idx_cond = idx if idx.size(1) <= self.config.context_length else idx[:, -self.config.context_length:]\n","            logits, _ = self(idx_cond)\n","            logits = logits[:, -1, :] / temperature\n","\n","            if top_k is not None:\n","                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n","                logits[logits < v[:, [-1]]] = -float('Inf')\n","\n","            probs = F.softmax(logits, dim=-1)\n","            idx_next = torch.multinomial(probs, num_samples=1)\n","            idx = torch.cat((idx, idx_next), dim=1)\n","\n","        return idx"],"metadata":{"id":"ygHw7jcd6suX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Training time\n","\n","gc.collect()\n","# -----------------------------------------------------------------------------\n","# default config values designed to train a gpt on hg38 reference genome\n","# I/O\n","out_dir = os.path.join(DIR, \"out\")\n","eval_interval = 1000\n","log_interval = 100\n","eval_iters = 100\n","eval_only = False # if True, script exits right after the first eval\n","always_save_checkpoint = False # if True, always save a checkpoint after each eval\n","init_from = 'scratch' # 'scratch' or 'resume' or 'gpt2*'\n","# wandb logging\n","wandb_log = False # disabled by default\n","wandb_project = 'genome_char'\n","wandb_run_name = 'gph-colab-' + str(int(time.time()))\n","# data\n","dataset = 'genome_char'\n","gradient_accumulation_steps = 1 # used to simulate larger batch sizes\n","context_length = 256\n","# model\n","batch_size=64\n","bias = False # do we use bias inside LayerNorm and Linear layers?\n","# adamw optimizer\n","max_iters = 3000 # total number of training iterations\n","grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n","# learning rate decay settings\n","learning_rate=6e-4\n","decay_lr = True # whether to decay the learning rate\n","weight_decay = 0.4\n","warmup_iters = 100 # how many steps to warm up for\n","lr_decay_iters = 3000 # should be ~= max_iters per Chinchilla\n","min_lr = 1e-4 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n","# DDP settings\n","#backend = 'nccl' # 'nccl', 'gloo', etc.\n","# system\n","device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n","dtype = 'float16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n","if torch.cuda.is_available() and device == 'cuda':\n","    print('cuda')\n","else:\n","    print('CPU')\n","compile = True # use PyTorch 2.0 to compile the model to be faster\n","# -----------------------------------------------------------------------------\n","config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n","#exec(open('configurator.py').read()) # overrides from command line or config file\n","config = {k: globals()[k] for k in config_keys} # will be useful for logging\n","# -----------------------------------------------------------------------------\n","\n","# various inits, derived attributes, I/O setup\n","master_process = True\n","seed_offset = 0\n","ddp_world_size = 1\n","tokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * context_length\n","print(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n","\n","if master_process:\n","    os.makedirs(out_dir, exist_ok=True)\n","torch.manual_seed(1337 + seed_offset)\n","torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n","torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n","device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n","# note: float16 data type will automatically use a GradScaler\n","ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n","ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n","\n","# poor man's data loader\n","data_dir = os.path.join(DIR, \"data\")\n","def get_batch(split):\n","    # We recreate np.memmap every batch to avoid a memory leak, as per\n","    # https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122\n","    if split == 'train':\n","        data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n","    else:\n","        data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n","    ix = torch.randint(len(data) - context_length, (batch_size,))\n","    x = torch.stack([torch.from_numpy((data[i:i+context_length]).astype(np.int64)) for i in ix])\n","    y = torch.stack([torch.from_numpy((data[i+1:i+1+context_length]).astype(np.int64)) for i in ix])\n","    if device_type == 'cuda':\n","        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n","        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n","    else:\n","        x, y = x.to(device), y.to(device)\n","    return x, y\n","\n","# init these up here, can override if init_from='resume' (i.e. from a checkpoint)\n","iter_num = 0\n","best_val_loss = 1e9\n","\n","# attempt to derive vocab_size from the dataset (should be 4 in the case of DNA data)\n","meta_path = os.path.join(data_dir, 'meta.pkl')\n","meta_vocab_size = None\n","if os.path.exists(meta_path):\n","    with open(meta_path, 'rb') as f:\n","        meta = pickle.load(f)\n","    meta_vocab_size = meta['vocab_size']\n","    print(f\"found vocab_size = {meta_vocab_size} (inside {meta_path})\")\n","\n","gphconf = Config(\n","  d_model=386,\n","  n_layers=4,\n","  vocab_size=meta_vocab_size if meta_vocab_size is not None else 40,\n","  d_embed=33,\n","  d_filter_mlp=64,\n","  n_filter_layers=4,\n","  context_length=context_length,\n","  short_conv_size=3,\n","  order=2,\n","  pdrop_hyena=0.0,\n","  pdrop_embed=0.2,\n","  omega=12,\n","  epochs=40,\n","  learning_rate=learning_rate,\n","  betas=(0.9, 0.98),\n","  weight_decay=weight_decay,\n","  device_type=device,\n","  batch_size=64,\n","  bias = False\n",")\n","\n"," # start with model_args from command line\n","if init_from == 'scratch':\n","    print(\"Initializing a new Hyena model from scratch\")\n","    model = GPH(gphconf)\n","\n","model.to(device)\n","# initialize a GradScaler. If enabled=False scaler is a no-op\n","scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n","\n","# optimizer\n","optimizer = model.configure_optimizers(\n","    weight_decay=weight_decay,\n","    learning_rate=learning_rate,\n","    betas=(0.9, 0.98),\n","    device_type=device_type\n",")\n","checkpoint = None # free up memory\n","\n","# torch compile\n","if compile:\n","    print(\"compiling the model... (takes ~1 min)\")\n","    raw_model = model\n","    model = torch.compile(model)\n","\n","# helps estimate an arbitrarily accurate loss over either split using many batches\n","@torch.no_grad()\n","def estimate_loss():\n","    out = {}\n","    model.eval()\n","    for split in ['train', 'val']:\n","        losses = torch.zeros(eval_iters)\n","        for k in range(eval_iters):\n","            X, Y = get_batch(split)\n","            with ctx:\n","                logits, loss = model(X, Y)[:2]\n","            losses[k] = loss.item()\n","        out[split] = losses.mean()\n","    model.train()\n","    return out\n","\n","# learning rate decay scheduler (cosine with warmup)\n","def get_lr(it):\n","    # 1) linear warmup for warmup_iters steps\n","    if it < warmup_iters:\n","        return learning_rate * it / warmup_iters\n","    # 2) if it > lr_decay_iters, return min learning rate\n","    if it > lr_decay_iters:\n","        return min_lr\n","    # 3) in between, use cosine decay down to min learning rate\n","    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n","    assert 0 <= decay_ratio <= 1\n","    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n","    return min_lr + coeff * (learning_rate - min_lr)\n","\n","# logging\n","if wandb_log and master_process:\n","    import wandb\n","    wandb.init(project=wandb_project, name=wandb_run_name, config=config, dir = os.path.join(DIR, \"log\"))\n","\n","# training loop\n","X, Y = get_batch('train') # fetch the very first batch\n","t0 = time.time()\n","local_iter_num = 0 # number of iterations in the lifetime of this process\n","raw_model = model\n","running_mfu = -1.0\n","while True:\n","\n","    # determine and set the learning rate for this iteration\n","    lr = get_lr(iter_num) if decay_lr else learning_rate\n","    for param_group in optimizer.param_groups:\n","        param_group['lr'] = lr\n","\n","    # evaluate the loss on train/val sets and write checkpoints\n","    if iter_num % eval_interval == 0 and master_process:\n","        losses = estimate_loss()\n","        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n","        if wandb_log:\n","            wandb.log({\n","                \"iter\": iter_num,\n","                \"train/loss\": losses['train'],\n","                \"val/loss\": losses['val'],\n","                \"lr\": lr,\n","                \"mfu\": running_mfu*100, # convert to percentage\n","            })\n","        if losses['val'] < best_val_loss or always_save_checkpoint:\n","            best_val_loss = losses['val']\n","            if iter_num > 0:\n","                checkpoint = {\n","                    'model': raw_model.state_dict(),\n","                    'optimizer': optimizer.state_dict(),\n","                    'model_args': asdict(gphconf),\n","                    'iter_num': iter_num,\n","                    'best_val_loss': best_val_loss,\n","                    'config': config,\n","                }\n","                print(f\"saving checkpoint to {out_dir}\")\n","                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n","    if iter_num == 0 and eval_only:\n","        break\n","\n","    # forward backward update, with optional gradient accumulation to simulate larger batch size\n","    # and using the GradScaler if data type is float16\n","    for micro_step in range(gradient_accumulation_steps):\n","        with ctx:\n","            logits, loss = model(X, Y)\n","            loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n","        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n","        X, Y = get_batch('train')\n","        # backward pass, with gradient scaling if training in fp16\n","        scaler.scale(loss).backward()\n","    # clip the gradient\n","    if grad_clip != 0.0:\n","        scaler.unscale_(optimizer)\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n","    # step the optimizer and scaler if training in fp16\n","    scaler.step(optimizer)\n","    scaler.update()\n","    # flush the gradients as soon as we can, no need for this memory anymore\n","    optimizer.zero_grad(set_to_none=True)\n","\n","    # timing and logging\n","    t1 = time.time()\n","    dt = t1 - t0\n","    t0 = t1\n","    if iter_num % log_interval == 0 and master_process:\n","        # get loss as float. note: this is a CPU-GPU sync point\n","        # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n","        lossf = loss.item() * gradient_accumulation_steps\n","        if local_iter_num >= 5: # let the training loop settle a bit\n","            mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n","            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n","        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n","    iter_num += 1\n","    local_iter_num += 1\n","\n","    # termination conditions\n","    if iter_num > max_iters:\n","        break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1-evjbT76wbY","executionInfo":{"status":"ok","timestamp":1747512877975,"user_tz":300,"elapsed":766801,"user":{"displayName":"Riley Xin","userId":"01480201500918314054"}},"outputId":"5b5e8b9f-fca2-4641-ebfb-26555482bfc3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n","tokens per iteration will be: 16,384\n","found vocab_size = 40 (inside /content/drive/MyDrive/UChicago/Spring2025/GENE46100/project/data/meta.pkl)\n","Initializing a new Hyena model from scratch\n","number of parameters: 8.09M\n","num decayed parameter tensors: 38, with 8,076,664 parameters\n","num non-decayed parameter tensors: 21, with 14,282 parameters\n","using fused AdamW: True\n","compiling the model... (takes ~1 min)\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-15-a06763f74933>:129: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n"]},{"output_type":"stream","name":"stdout","text":["step 0: train loss 3.7035, val loss 3.6984\n","iter 0: loss 3.7042, time 116724.26ms, mfu -100.00%\n","iter 100: loss 2.2562, time 202.00ms, mfu 1.39%\n","iter 200: loss 1.5458, time 190.50ms, mfu 1.39%\n","iter 300: loss 1.4356, time 187.66ms, mfu 1.40%\n","iter 400: loss 1.3567, time 189.51ms, mfu 1.41%\n","iter 500: loss 1.4302, time 192.94ms, mfu 1.41%\n","iter 600: loss 1.3574, time 190.57ms, mfu 1.42%\n","iter 700: loss 1.4423, time 188.92ms, mfu 1.43%\n","iter 800: loss 1.3711, time 189.68ms, mfu 1.43%\n","iter 900: loss 1.3690, time 189.08ms, mfu 1.44%\n","step 1000: train loss 1.3698, val loss 1.2614\n","saving checkpoint to /content/drive/MyDrive/UChicago/Spring2025/GENE46100/project/out\n","iter 1000: loss 1.3796, time 29874.34ms, mfu 1.29%\n","iter 1100: loss 1.3784, time 196.40ms, mfu 1.31%\n","iter 1200: loss 1.4161, time 193.62ms, mfu 1.32%\n","iter 1300: loss 1.3839, time 190.15ms, mfu 1.34%\n","iter 1400: loss 1.3562, time 186.61ms, mfu 1.35%\n","iter 1500: loss 1.3761, time 189.25ms, mfu 1.36%\n","iter 1600: loss 1.3361, time 193.85ms, mfu 1.37%\n","iter 1700: loss 1.3282, time 192.17ms, mfu 1.38%\n","iter 1800: loss 1.3259, time 188.36ms, mfu 1.39%\n","iter 1900: loss 1.2557, time 190.88ms, mfu 1.40%\n","step 2000: train loss 1.2877, val loss 1.1821\n","saving checkpoint to /content/drive/MyDrive/UChicago/Spring2025/GENE46100/project/out\n","iter 2000: loss 1.2912, time 21523.76ms, mfu 1.26%\n","iter 2100: loss 1.3262, time 196.39ms, mfu 1.28%\n","iter 2200: loss 1.2708, time 192.15ms, mfu 1.29%\n","iter 2300: loss 1.3284, time 188.45ms, mfu 1.31%\n","iter 2400: loss 1.3015, time 189.31ms, mfu 1.33%\n","iter 2500: loss 1.2929, time 192.41ms, mfu 1.34%\n","iter 2600: loss 1.2780, time 191.25ms, mfu 1.35%\n","iter 2700: loss 1.2907, time 188.82ms, mfu 1.37%\n","iter 2800: loss 1.3168, time 192.72ms, mfu 1.38%\n","iter 2900: loss 1.2686, time 188.66ms, mfu 1.39%\n","step 3000: train loss 1.2649, val loss 1.1618\n","saving checkpoint to /content/drive/MyDrive/UChicago/Spring2025/GENE46100/project/out\n","iter 3000: loss 1.2102, time 20749.65ms, mfu 1.25%\n"]}]},{"cell_type":"code","source":["#@title Sampling from the pre-trained model\n","# Generating config\n","init_from = 'resume'\n","DIR = \"/content/drive/MyDrive/UChicago/Spring2025/GENE46100/project/\"\n","out_dir = os.path.join(DIR, \"out\")\n","meta_path = os.path.join(DIR, \"data\", \"meta.pkl\")\n","start = \"\\n\"  # initial prompt\n","num_samples = 5\n","max_new_tokens = 250\n","temperature = 0.8\n","top_k = 200\n","seed = 1337\n","device = 'cuda'\n","dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\n","compile = True\n","# -----------------------------------------------------------------------------\n","\n","# Setup\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)\n","torch.backends.cuda.matmul.allow_tf32 = True\n","torch.backends.cudnn.allow_tf32 = True\n","device_type = 'cuda' if 'cuda' in device else 'cpu'\n","ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n","ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n","\n","# Load checkpoint\n","ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n","checkpoint = torch.load(ckpt_path, map_location=device)\n","model_args = checkpoint['model_args']\n","gphconf = Config(**model_args)\n","model = GPH(gphconf)\n","\n","# Handle '_orig_mod.' prefix from torch.compile() in saved state_dict\n","state_dict = checkpoint['model']\n","unwanted_prefix = '_orig_mod.'\n","for k in list(state_dict.keys()):\n","    if k.startswith(unwanted_prefix):\n","        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n","\n","model.load_state_dict(state_dict)\n","model.eval().to(device)\n","\n","if compile:\n","    model = torch.compile(model)\n","\n","# Load vocab (stoi, itos)\n","with open(meta_path, 'rb') as f:\n","    meta = pickle.load(f)\n","stoi, itos = meta['stoi'], meta['itos']\n","encode = lambda s: [stoi[c] for c in s]\n","decode = lambda l: ''.join([itos[i] for i in l])\n","\n","# Encode prompt\n","if start.startswith('FILE:'):\n","    with open(start[5:], 'r', encoding='utf-8') as f:\n","        start = f.read()\n","start_ids = encode(start)\n","x = torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...]\n","\n"],"metadata":{"id":"mHKuoogYJmkl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Generate sequences\n","with torch.no_grad():\n","    with ctx:\n","        for k in range(num_samples):\n","            y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n","            print(decode(y[0].tolist()))\n","            print('---------------')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yy4G2zWV6M6h","executionInfo":{"status":"ok","timestamp":1747513330717,"user_tz":300,"elapsed":10387,"user":{"displayName":"Riley Xin","userId":"01480201500918314054"}},"outputId":"3f0ea4bd-1b51-42bb-b5dd-8914019cb4e4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","AAAAACACTTGAAAAAGGACACCCCCAAATAAAGCAATGCGACCTGCTCT\n","TATGTAGATTTTACAATTCCAGGGTCCATTCAGGTCAAAGATGATTGTTG\n","TCCTACAAGTAATTCCAAATGCACCCTCTGGCACATTTTCATTCTGAGTG\n","TACAGATATAATCTTGAAAATGATAGAATCATTAttttttttttattatt\n","tttaattcatttataaactttaaaggtttttttttggaattgtatt\n","---------------\n","\n","TATGTGAACAAAACTTCTTAAATTATGTAAAATGTGATTCTTTTTTATGT\n","AGTTATTCTATTTTTCATTTCTGAAAAAATAAAAATTTGATTTTGTGAAA\n","TTAAAAAAATCACAGATTAATTTTCTGAAAAAATTGGCCATTTTTAAACA\n","AAATTTTTTTTATTCTTCAATTTTTAATTTTTTAACATATAAAAATATAA\n","ATGTTTATTGAGTAATTTAATAACAATCACTTTAAATTTTTTTTAA\n","---------------\n","\n","ATAACATCAGTAAATTTAAAAATTATTGGCTTTTTCATCTGTGT\n","ATATT\n","GGCTTCCATCCTGTATGGTAGTCTTTGAGAAATATTTCACTTAT\n","GGAGA\n","CGATTCCATTTCCATTATTAGTGGGCTATCTGGTTTTCTTTTTC\n","TTTTT\n","AAATTTAGATctttatatttttttggctttgtgttttattttat\n","ttttt\n","ttttttttttatattttttttttacatttatttaattgcaatga\n","a\n","---------------\n","\n","cccagccttcatttcctgagag\n","ttttgaatttctgtttttttttcttga\n","ttgaaacagagtattcctttct\n","taaataggatctcaagagttaaaCTGT\n","CTTTGACAAAAATTTTATCTCC\n","TAAGTTGTCTTTATTCTGATGTAAAAA\n","TTTATATTTTTTTATGTGATAG\n","ATTGTTATTAATTTAACTTCTTTAATT\n","GTTTTTTTTATTTTATAATTTT\n","ATAATTTTTAAATTAATGCTCAA\n","---------------\n","\n","TGCTTGGTTAACTGACTGTA\n","CCAAAGGTTTCTTTTCTTCATAATTTTGA\n","ATTACTTCATTTCAGTTTCC\n","ATTCTGTGATTGTTTTAAGAAGTCTTCAA\n","CCCAGAGCAGTCCCCTACAA\n","CATGTTGTTTTTACTGCTTAGAATATGAT\n","ACATTTTGAAGGCGTTTGCT\n","ATCCAAAGCAACCCTTGTGTCTTTTACAT\n","CTTATTAGTCATATATGTCA\n","GTGATTTTACATGAATTTTTAGGTA\n","---------------\n"]}]},{"cell_type":"markdown","source":["# Fine-tune the model for enhancer classification"],"metadata":{"id":"0ZogBZT0P-b-"}},{"cell_type":"markdown","source":["As a downstream application of the GPH model, I evaluated its performance on a three-class enhancer classification task that was introduced in class using the nano-GPT framework. This task involves predicting the enhancer type from raw DNA sequence input. Details on the dataset are available on [Hugging Face Datasets](https://huggingface.co/datasets/InstaDeepAI/nucleotide_transformer_downstream_tasks)."],"metadata":{"id":"vu79cKF-US54"}},{"cell_type":"code","source":["#@title Define the model for multi-task classification\n","class ClassificationModel(nn.Module):\n","    def __init__(self, base_model, num_labels):\n","        super().__init__()\n","        self.base_model = base_model\n","        self.classifier = nn.Linear(base_model.config.d_model, num_labels)\n","        self.num_labels = num_labels\n","\n","    def forward(self, input_ids, labels=None):\n","        # get the output of the base model before the language modeling head\n","        b, t = input_ids.size()\n","        assert t <= self.base_model.config.context_length, f\"Cannot forward sequence of length {t}, context length is only {self.base_model.config.context_length}\"\n","\n","        x = self.base_model.tok_emb(input_ids) + self.base_model.pos_emb[:, :t, :]\n","        x = self.base_model.drop(x)\n","        x = self.base_model.blocks(x)\n","        pooled_output = self.base_model.ln_f(x).mean(dim=1)\n","\n","        class_logits = self.classifier(pooled_output)\n","\n","        loss = None\n","        if labels is not None:\n","            loss = F.cross_entropy(class_logits, labels)\n","\n","        return (class_logits, loss) if loss is not None else class_logits"],"metadata":{"id":"8ugFYPteP9zI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the enhancer dataset from the InstaDeep Hugging Face ressources\n","dataset_name = \"enhancers\"\n","train_dataset_enhancers = load_dataset(\n","        \"InstaDeepAI/nucleotide_transformer_downstream_tasks\",\n","        dataset_name,\n","        split=\"train\",\n","        streaming=False\n","    )\n","test_dataset_enhancers = load_dataset(\n","        \"InstaDeepAI/nucleotide_transformer_downstream_tasks\",\n","        dataset_name,\n","        split=\"test\",\n","        streaming=False\n","    )\n","\n","num_labels_enhancer = 3"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":651,"referenced_widgets":["cc3c76b8d48a4d7b97d20eaa2b2de012","5761a00b16b3422d92bee850cff99cf9","3e6a2ef5f08a421abea9715d1b73087f","487cec60bb2040069e2076879f9a69c8","a6b44322c24a4ee0badf610488e983e6","bc4e17620fb940eb93f2b627c960a832","1b9c2330761e48038ef682b3b8026c5e","cac59a2331b24da8b43b59fe2d530a94","30f01b9e260d4f9b87771519ce2a6a31","daf9673f61024b35be693a492e61b666","aabec609e5b14e76bcc560cdd042a83e","dc09358245924b6eb597334b865b0e1a","d9bf6d6ee8c7405195480041977ccfcf","4ebfe06b44704b9b9b73d5b94a420b27","e65fe4110c6e4811b2fa411719c3f3ec","f8f70c9533264b21886ce1572861dd75","c0c306c859424a0e830372551ce72704","84829e6d78cf46519188093df64108fa","e306eedbb97d4674a494fb36e384cebf","22bce4fb14714c4d86af868237a9e1e8","bbf8ee325aaf4953a7998b7535c9b97b","fff2b74a0a314d5c8b42ec56eef354ae","54264ccd737046618ba2a26e8eb7246b","d5665a7045724b599cbee61986d07bb9","3901f6cfb4974ce4bca6417529a679a2","235ce30b272d4e7e8f06ed58a11646d3","dd85fab528e34220bd79d7fddc958e10","5312a3a9e66d48d0842f249ec8228565","9dd7e452baf9402495c489095e2045b6","56d2f82ba6d641979a714fa36948aa76","5daef6d11c6e422db99ad239ab4d0a89","698ba275885e4e09bde471f044cd8bf9","fd0b584c87ff43ccb94ede84cc2d68be","d0454bae563545fc87e77af8522dfd49","801406babaaf4de8a91644561f34961d","b3bdddc8789548ff9d7d5aff2ecbb407","c7f28e4b7dea4e79a4a9a19dd7bc5266","810137fe0ea84edba1bc7d2b99263010","58ecc1ac083a4b35bfbf2eebac57d44e","a222ef5d456f4011af8b3d999401a379","cf12cac7efe84db9b799a06336959d7c","1b150db0e7114291bea75343fe4ef000","25e3f8ff94c54445bdfa5e4ee898874f","ef374a1857ca430c86bf09dedf9bfa70"]},"collapsed":true,"id":"0DA8pTcUQPJz","executionInfo":{"status":"ok","timestamp":1747613973168,"user_tz":300,"elapsed":2792,"user":{"displayName":"Riley Xin","userId":"01480201500918314054"}},"outputId":"702c5588-bd06-410c-dfc4-690910015c45"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["train.fna:   0%|          | 0.00/3.10M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc3c76b8d48a4d7b97d20eaa2b2de012"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["test.fna:   0%|          | 0.00/83.2k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc09358245924b6eb597334b865b0e1a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54264ccd737046618ba2a26e8eb7246b"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":[" \n","                       \n","        WARNING: Please note that the Nucleotide Transformer benchmark datasets have been revised\n","        during the per-review process. This version is deprecated and the new datasets are available at \n","        InstaDeepAI/nucleotide_transformer_downstream_tasks_revised.\n","                       \n","              \n","WARNING:datasets: \n","                       \n","        WARNING: Please note that the Nucleotide Transformer benchmark datasets have been revised\n","        during the per-review process. This version is deprecated and the new datasets are available at \n","        InstaDeepAI/nucleotide_transformer_downstream_tasks_revised.\n","                       \n","              \n"]},{"output_type":"display_data","data":{"text/plain":["Generating test split: 0 examples [00:00, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0454bae563545fc87e77af8522dfd49"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":[" \n","                       \n","        WARNING: Please note that the Nucleotide Transformer benchmark datasets have been revised\n","        during the per-review process. This version is deprecated and the new datasets are available at \n","        InstaDeepAI/nucleotide_transformer_downstream_tasks_revised.\n","                       \n","              \n","WARNING:datasets: \n","                       \n","        WARNING: Please note that the Nucleotide Transformer benchmark datasets have been revised\n","        during the per-review process. This version is deprecated and the new datasets are available at \n","        InstaDeepAI/nucleotide_transformer_downstream_tasks_revised.\n","                       \n","              \n"]}]},{"cell_type":"code","source":["# Get training data\n","train_sequences_enhancers = train_dataset_enhancers['sequence']\n","train_labels_enhancers = train_dataset_enhancers['label']\n","\n","# Split the dataset into a training and a validation dataset\n","train_sequences_enhancers, validation_sequences_enhancers, train_labels_enhancers, validation_labels_enhancers = train_test_split(train_sequences_enhancers,\n","                                                                              train_labels_enhancers, test_size=0.10, random_state=42)\n","\n","# Get test data\n","test_sequences_enhancers = test_dataset_enhancers['sequence']\n","test_labels_enhancers = test_dataset_enhancers['label']\n","\n","# Augument data to include reverse complement\n","\n","def reverse_complement(seq):\n","    complement = {'A': 'T', 'T': 'A', 'C': 'G', 'G': 'C'}\n","    return ''.join(complement.get(base, base) for base in reversed(seq))\n","\n","augmented_sequences = []\n","augmented_labels = []\n","\n","for seq, label in zip(train_sequences_enhancers, train_labels_enhancers):\n","    augmented_sequences.append(seq)\n","    augmented_labels.append(label)\n","\n","    rc_seq = reverse_complement(seq)\n","    augmented_sequences.append(rc_seq)\n","    augmented_labels.append(label)\n","\n","ds_train_enhancers = Dataset.from_dict({\"data\": augmented_sequences,'labels':augmented_labels})\n","ds_validation_enhancers = Dataset.from_dict({\"data\": validation_sequences_enhancers,'labels':validation_labels_enhancers})\n","ds_test_enhancers = Dataset.from_dict({\"data\": test_sequences_enhancers,'labels':test_labels_enhancers})"],"metadata":{"id":"0c3w60iZQVfV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def tokenize_function(examples):\n","  min_length = min(len(i) for i in examples['data'])\n","  outputs = np.empty((0, min_length), dtype='int16')\n","  for example in examples[\"data\"]:\n","    outputs = np.vstack([outputs, encode(example[:min_length])])\n","  return {\n","      'input_ids': outputs\n","  }\n","\n","tokenized_datasets_train_enhancer = ds_train_enhancers.map(\n","    tokenize_function,\n","    batched=True,\n","    remove_columns=['data'],\n",")\n","tokenized_datasets_validation_enhancer = ds_validation_enhancers.map(\n","    tokenize_function,\n","    batched=True,\n","    remove_columns=['data'],\n",")\n","tokenized_datasets_test_enhancer = ds_test_enhancers.map(\n","    tokenize_function,\n","    batched=True,\n","    remove_columns=['data'],\n",")\n","\n","device = 'cuda'\n","train_dataset = TensorDataset(torch.tensor(tokenized_datasets_train_enhancer['input_ids'], device=device),\n","                              torch.tensor(tokenized_datasets_train_enhancer['labels'], device=device))\n","train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=64)\n","\n","validation_dataset = TensorDataset(torch.tensor(tokenized_datasets_validation_enhancer['input_ids'], device=device),\n","                              torch.tensor(tokenized_datasets_validation_enhancer['labels'], device=device))\n","\n","validation_dataloader = DataLoader(validation_dataset, shuffle=True, batch_size=1)\n","\n","test_dataset = TensorDataset(torch.tensor(tokenized_datasets_test_enhancer['input_ids'], device=device),\n","                              torch.tensor(tokenized_datasets_test_enhancer['labels'], device=device))\n","\n","test_dataloader = DataLoader(test_dataset, shuffle=True, batch_size=1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":113,"referenced_widgets":["770f0910475c48ddb761491fbd2d1a6f","1daedb63a46f44848dec54c1cd1fa1ef","c7ddc98f8ed24757b759e535a2e7be32","408bf721d0a841f194ac140fc0011820","9c7f35f641d549bb885f43c981635eb7","f4c24864a8334ff9938cca4d72de6951","098fb1fa92404f6e8d9b86bba36bff4b","7ccf43ed79c04b4e8daca192207d0126","85799eb3f579487894ba777813aa063d","311c108c8e6d46719cacda46ae25901b","5e2ca5f227d74f6a8e7486a233fe7c9d","bff9d7f6ba93401a97d51702e6c258f9","daa732720a314b1882722a36c68371f7","027b16ee040c446a8d32bb3f9c48ec1c","0fd40c06ba1f4718b725570e3a558cf0","d405f6adbb654db39bc268ec404498d6","2eeab57bd0f5414481d631bde80326f5","14bf3a1e10954c64a726f4c0013cbece","9e4308e982a5460ab3cc1f0dbc2e915c","166f31f9cc3944ce9659611414987e49","3255a135111547e9ba76470cf2a91697","02e6b0919cf84656928cd09837d5505c","209966e32d354f7bbf5ea7005eca81d3","354e10d60bec4b50b8b9a14a0e80b65b","436674a570984df5a79b83595a5662e0","9ae43e89c28f48f6bbd04a4008b71602","585c0cde639f48abb5eb1865056b7dfa","313772ac55a4406a9e39a5e5b283e783","5989681f85414248ad8d870290bd49e9","e5d2e82dff3f403d97958d385725a3c7","e32e59db64cf43d0b486e230651c2e33","8a5cca531866491b867cbb48833a5846","89165379d9ec49c99941bac9ede813d4"]},"id":"wU7SuBlpQhUW","executionInfo":{"status":"ok","timestamp":1747614061918,"user_tz":300,"elapsed":6963,"user":{"displayName":"Riley Xin","userId":"01480201500918314054"}},"outputId":"0eadb168-dbeb-43bf-fd66-e2a0c8268bb9"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/26942 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"770f0910475c48ddb761491fbd2d1a6f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/1497 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bff9d7f6ba93401a97d51702e6c258f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/400 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"209966e32d354f7bbf5ea7005eca81d3"}},"metadata":{}}]},{"cell_type":"code","source":["# save the tensors\n","save_path = os.path.join(DIR, \"data\")\n","torch.save(train_dataset.tensors, os.path.join(save_path, \"enhancer_train_tensors.pt\"))\n","torch.save(validation_dataset.tensors, os.path.join(save_path, \"enhancer_val_tensors.pt\"))\n","torch.save(test_dataset.tensors, os.path.join(save_path, \"enhancer_test_tensors.pt\"))"],"metadata":{"id":"TmDoZulF7CiE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Training time\n","# -----------------------------------------------------------------------------\n","init_from = 'resume'\n","out_dir = os.path.join(DIR, \"out\") # ignored if init_from is not 'resume'\n","seed = 1337\n","device = 'cuda'\n","dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\n","compile = True # use PyTorch 2.0 to compile the model to be faster\n","# -----------------------------------------------------------------------------\n","\n","torch.cuda.init()\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)\n","torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n","torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n","device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n","ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}['float16']\n","ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n","\n","# model\n","if init_from == 'resume':\n","    ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n","    checkpoint = torch.load(ckpt_path, map_location=device)\n","    model_args = checkpoint['model_args']\n","    model_args['d_embed'] = model_args['d_model'] #???????\n","    gphconf = Config(**model_args)\n","    model = GPH(gphconf)\n","\n","    # Handle '_orig_mod.' prefix from torch.compile() in saved state_dict\n","    state_dict = checkpoint['model']\n","    unwanted_prefix = '_orig_mod.'\n","    for k in list(state_dict.keys()):\n","        if k.startswith(unwanted_prefix):\n","            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n","\n","    model.load_state_dict(state_dict)\n","\n","\n","# ========================== NEW MODEL SETUP ================================\n","new_model = ClassificationModel(base_model=model, num_labels=3)\n","new_model.eval()\n","new_model.to(device)\n","if compile:\n","    new_model = torch.compile(new_model)  # requires PyTorch 2.0 (optional)\n","\n","optimizer = torch.optim.AdamW(new_model.parameters(), lr=2e-5)\n","num_epochs = 10\n","total_steps = len(train_dataloader) * num_epochs\n","\n","val_predictions = []\n","val_true_labels = []\n","\n","# ========================== TRAINING LOOP ================================\n","loss_lst = []\n","val_loss_lst = []\n","mcc_scores = []\n","new_model.train()\n","for epoch in range(num_epochs):\n","    i = 0\n","    for batch in train_dataloader:\n","        input_ids, labels = batch\n","        optimizer.zero_grad()\n","\n","        # Forward pass\n","        outputs = new_model(input_ids, labels=labels)\n","        loss = outputs[1]\n","        loss_lst.append(loss.item())\n","\n","        # Backward pass and optimization\n","        loss.backward()\n","        optimizer.step()\n","        i += 1\n","\n","        if i % 100 == 99:\n","            new_model.eval()\n","            with torch.no_grad():\n","                for val_batch in validation_dataloader:\n","                    val_input_ids, val_labels = val_batch\n","                    val_outputs = new_model(val_input_ids, labels=val_labels)\n","                    val_loss = val_outputs[1]\n","                    val_loss_lst.append(val_loss.item())\n","                    val_logits = val_outputs[0]\n","                    val_predictions.append(torch.argmax(val_logits).item())\n","                    val_true_labels.append(val_labels.item())\n","\n","            print(f\"Epoch {epoch}, Batch {i}, Train Loss: {np.mean(loss_lst)}, Val Loss: {np.mean(val_loss_lst)}, Val MCC: {matthews_corrcoef(val_true_labels, val_predictions)}\")\n","            mcc_scores.append(matthews_corrcoef(val_true_labels, val_predictions))\n","            loss_lst = []\n","            val_loss_lst = []\n","            new_model.train()\n","            val_predictions = []\n","            val_true_labels = []\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8WMnT5ogQ29W","executionInfo":{"status":"ok","timestamp":1747616497724,"user_tz":300,"elapsed":1914522,"user":{"displayName":"Riley Xin","userId":"01480201500918314054"}},"outputId":"3712f094-d759-4972-d609-66161a7a8098"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["number of parameters: 8.09M\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/_inductor/lowering.py:1814: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0, Batch 99, Train Loss: 0.8663327416988335, Val Loss: 0.6295053677592345, Val MCC: 0.3023741385830867\n","Epoch 0, Batch 199, Train Loss: 0.5832807952165604, Val Loss: 0.5526515803210881, Val MCC: 0.44828390452342215\n","Epoch 0, Batch 299, Train Loss: 0.5364896994829178, Val Loss: 0.5604384505023847, Val MCC: 0.4506472491784239\n","Epoch 0, Batch 399, Train Loss: 0.5360657167434693, Val Loss: 0.6257648226987315, Val MCC: 0.3854330836666037\n","Epoch 1, Batch 99, Train Loss: 0.5267509098880547, Val Loss: 0.564268953591192, Val MCC: 0.4558687106175817\n","Epoch 1, Batch 199, Train Loss: 0.5365271699428559, Val Loss: 0.6142528217283383, Val MCC: 0.4008737129321\n","Epoch 1, Batch 299, Train Loss: 0.5305521339178085, Val Loss: 0.5679025071167306, Val MCC: 0.4541016710293508\n","Epoch 1, Batch 399, Train Loss: 0.522105237543583, Val Loss: 0.5624742892812801, Val MCC: 0.4606540166790629\n","Epoch 2, Batch 99, Train Loss: 0.5248378199979293, Val Loss: 0.6294540409956801, Val MCC: 0.38399433671075767\n","Epoch 2, Batch 199, Train Loss: 0.523768892288208, Val Loss: 0.6119219349997719, Val MCC: 0.39596816402073676\n","Epoch 2, Batch 299, Train Loss: 0.5036548069119453, Val Loss: 0.6227136238256648, Val MCC: 0.4044413056995183\n","Epoch 2, Batch 399, Train Loss: 0.5197805508971214, Val Loss: 0.5968008441430608, Val MCC: 0.40584165500632846\n","Epoch 3, Batch 99, Train Loss: 0.5147248949886354, Val Loss: 0.5649742601204816, Val MCC: 0.45317203616299595\n","Epoch 3, Batch 199, Train Loss: 0.5136365893483162, Val Loss: 0.524785274799301, Val MCC: 0.4983186346091211\n","Epoch 3, Batch 299, Train Loss: 0.49702319979667664, Val Loss: 0.6576911169252033, Val MCC: 0.37735002699740766\n","Epoch 3, Batch 399, Train Loss: 0.48403234899044034, Val Loss: 0.5774586850451585, Val MCC: 0.45675075041624635\n","Epoch 4, Batch 99, Train Loss: 0.4901416690881587, Val Loss: 0.5585155257256547, Val MCC: 0.4670148422562411\n","Epoch 4, Batch 199, Train Loss: 0.4756994253396988, Val Loss: 0.5069785322309656, Val MCC: 0.5205980196310277\n","Epoch 4, Batch 299, Train Loss: 0.46511262983083723, Val Loss: 0.5352347077884722, Val MCC: 0.4993675836211688\n","Epoch 4, Batch 399, Train Loss: 0.4681324923038483, Val Loss: 0.4501447450498221, Val MCC: 0.5947660373686408\n","Epoch 5, Batch 99, Train Loss: 0.45560622658611327, Val Loss: 0.46101711125978756, Val MCC: 0.5938557933126796\n","Epoch 5, Batch 199, Train Loss: 0.4559616297483444, Val Loss: 0.47626273876730146, Val MCC: 0.5805753908079653\n","Epoch 5, Batch 299, Train Loss: 0.43898621171712876, Val Loss: 0.45907425609752894, Val MCC: 0.598034246783732\n","Epoch 5, Batch 399, Train Loss: 0.44978013783693316, Val Loss: 0.5037861819028775, Val MCC: 0.5380080461994324\n","Epoch 6, Batch 99, Train Loss: 0.44026258981917515, Val Loss: 0.45998166589397355, Val MCC: 0.6029685227668734\n","Epoch 6, Batch 199, Train Loss: 0.42544679999351503, Val Loss: 0.49749801557807144, Val MCC: 0.5543992289276505\n","Epoch 6, Batch 299, Train Loss: 0.4249429327249527, Val Loss: 0.4789191248182238, Val MCC: 0.5866277436781567\n","Epoch 6, Batch 399, Train Loss: 0.4193906280398369, Val Loss: 0.5788442054436237, Val MCC: 0.50606746407624\n","Epoch 7, Batch 99, Train Loss: 0.41816432761751915, Val Loss: 0.5376326158905733, Val MCC: 0.5325537558655893\n","Epoch 7, Batch 199, Train Loss: 0.4202509915828705, Val Loss: 0.47515702198728305, Val MCC: 0.6052995652872171\n","Epoch 7, Batch 299, Train Loss: 0.4115170818567276, Val Loss: 0.47207923950246783, Val MCC: 0.6048075253373151\n","Epoch 7, Batch 399, Train Loss: 0.41065385460853576, Val Loss: 0.4898007186918137, Val MCC: 0.5843660389207035\n","Epoch 8, Batch 99, Train Loss: 0.4141900363293561, Val Loss: 0.47706408393356214, Val MCC: 0.6046954139741395\n","Epoch 8, Batch 199, Train Loss: 0.3938766178488731, Val Loss: 0.4973030064487266, Val MCC: 0.5774161975197115\n","Epoch 8, Batch 299, Train Loss: 0.4101472216844559, Val Loss: 0.4915756267128321, Val MCC: 0.5857497253051704\n","Epoch 8, Batch 399, Train Loss: 0.40213597267866136, Val Loss: 0.586103876101405, Val MCC: 0.4962083499309118\n","Epoch 9, Batch 99, Train Loss: 0.4078324551671004, Val Loss: 0.5335174646510374, Val MCC: 0.5473804802942817\n","Epoch 9, Batch 199, Train Loss: 0.39891387075185775, Val Loss: 0.5240813595432333, Val MCC: 0.5570577242936345\n","Epoch 9, Batch 299, Train Loss: 0.40093156903982163, Val Loss: 0.53805040518439, Val MCC: 0.548199146141425\n","Epoch 9, Batch 399, Train Loss: 0.3893241885304451, Val Loss: 0.629607133437824, Val MCC: 0.47646210854481674\n"]}]},{"cell_type":"code","source":["#@ Evaluation\n","new_model.eval()\n","\n","predictions = []\n","true_labels = []\n","i = 0\n","\n","with torch.no_grad():\n","    for batch in test_dataloader:\n","        input_ids, labels = batch\n","        outputs = new_model(input_ids)\n","        logits = outputs[0]\n","        predictions.append(torch.argmax(logits).item())\n","        true_labels.append(labels.item())\n","\n","correct = 0\n","print(len(predictions))\n","for i in np.arange(len(predictions)) :\n","    if predictions[i] == true_labels[i]:\n","        correct += 1\n","print(f\"Accuracy: {int(correct / len(predictions) * 100):,}%\")\n","\n","print(f\"MCC: {round(matthews_corrcoef(true_labels, predictions), 2):,}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gu3IQZ3uRW4j","executionInfo":{"status":"ok","timestamp":1747616531655,"user_tz":300,"elapsed":16455,"user":{"displayName":"Riley Xin","userId":"01480201500918314054"}},"outputId":"837381d0-9ee5-4bf7-fd38-3371c779b4e5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["400\n","Accuracy: 70%\n","MCC: 0.41\n"]}]},{"cell_type":"markdown","source":["# Conclusion"],"metadata":{"id":"N6KH2VgIZgTm"}},{"cell_type":"markdown","source":["The GPH framework outperforms the original GPT model presented in class, improving prediction accuracy from below 50% to 70% on the enhancer classification task. This improvement comes at the cost of increased complexity: my specific GPH has 8.09M parameters compared to 7.10M in the nano-GPT model and required roughly twice as long time to train, despite using a shorter context length (256 bp vs. 300 bp). This trade-off is consistent with expectations—Hyena operators tend to be slower than attention mechanisms at short sequence lengths but are designed to scale more efficiently with much longer contexts. Further empirical and theoretical work is needed to better understand how Hyena captures contextual information and why it yields better downstream performance in this setting."],"metadata":{"id":"KqeWzvf3XagI"}}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.9"},"widgets":{"application/vnd.jupyter.widget-state+json":{"cc3c76b8d48a4d7b97d20eaa2b2de012":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5761a00b16b3422d92bee850cff99cf9","IPY_MODEL_3e6a2ef5f08a421abea9715d1b73087f","IPY_MODEL_487cec60bb2040069e2076879f9a69c8"],"layout":"IPY_MODEL_a6b44322c24a4ee0badf610488e983e6"}},"5761a00b16b3422d92bee850cff99cf9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bc4e17620fb940eb93f2b627c960a832","placeholder":"​","style":"IPY_MODEL_1b9c2330761e48038ef682b3b8026c5e","value":"train.fna: 100%"}},"3e6a2ef5f08a421abea9715d1b73087f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cac59a2331b24da8b43b59fe2d530a94","max":3102309,"min":0,"orientation":"horizontal","style":"IPY_MODEL_30f01b9e260d4f9b87771519ce2a6a31","value":3102309}},"487cec60bb2040069e2076879f9a69c8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_daf9673f61024b35be693a492e61b666","placeholder":"​","style":"IPY_MODEL_aabec609e5b14e76bcc560cdd042a83e","value":" 3.10M/3.10M [00:00&lt;00:00, 26.8MB/s]"}},"a6b44322c24a4ee0badf610488e983e6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bc4e17620fb940eb93f2b627c960a832":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1b9c2330761e48038ef682b3b8026c5e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cac59a2331b24da8b43b59fe2d530a94":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"30f01b9e260d4f9b87771519ce2a6a31":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"daf9673f61024b35be693a492e61b666":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aabec609e5b14e76bcc560cdd042a83e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dc09358245924b6eb597334b865b0e1a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d9bf6d6ee8c7405195480041977ccfcf","IPY_MODEL_4ebfe06b44704b9b9b73d5b94a420b27","IPY_MODEL_e65fe4110c6e4811b2fa411719c3f3ec"],"layout":"IPY_MODEL_f8f70c9533264b21886ce1572861dd75"}},"d9bf6d6ee8c7405195480041977ccfcf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c0c306c859424a0e830372551ce72704","placeholder":"​","style":"IPY_MODEL_84829e6d78cf46519188093df64108fa","value":"test.fna: 100%"}},"4ebfe06b44704b9b9b73d5b94a420b27":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e306eedbb97d4674a494fb36e384cebf","max":83200,"min":0,"orientation":"horizontal","style":"IPY_MODEL_22bce4fb14714c4d86af868237a9e1e8","value":83200}},"e65fe4110c6e4811b2fa411719c3f3ec":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bbf8ee325aaf4953a7998b7535c9b97b","placeholder":"​","style":"IPY_MODEL_fff2b74a0a314d5c8b42ec56eef354ae","value":" 83.2k/83.2k [00:00&lt;00:00, 4.12MB/s]"}},"f8f70c9533264b21886ce1572861dd75":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c0c306c859424a0e830372551ce72704":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"84829e6d78cf46519188093df64108fa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e306eedbb97d4674a494fb36e384cebf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"22bce4fb14714c4d86af868237a9e1e8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bbf8ee325aaf4953a7998b7535c9b97b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fff2b74a0a314d5c8b42ec56eef354ae":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"54264ccd737046618ba2a26e8eb7246b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d5665a7045724b599cbee61986d07bb9","IPY_MODEL_3901f6cfb4974ce4bca6417529a679a2","IPY_MODEL_235ce30b272d4e7e8f06ed58a11646d3"],"layout":"IPY_MODEL_dd85fab528e34220bd79d7fddc958e10"}},"d5665a7045724b599cbee61986d07bb9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5312a3a9e66d48d0842f249ec8228565","placeholder":"​","style":"IPY_MODEL_9dd7e452baf9402495c489095e2045b6","value":"Generating train split: "}},"3901f6cfb4974ce4bca6417529a679a2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_56d2f82ba6d641979a714fa36948aa76","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5daef6d11c6e422db99ad239ab4d0a89","value":1}},"235ce30b272d4e7e8f06ed58a11646d3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_698ba275885e4e09bde471f044cd8bf9","placeholder":"​","style":"IPY_MODEL_fd0b584c87ff43ccb94ede84cc2d68be","value":" 14968/0 [00:00&lt;00:00, 26145.32 examples/s]"}},"dd85fab528e34220bd79d7fddc958e10":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5312a3a9e66d48d0842f249ec8228565":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9dd7e452baf9402495c489095e2045b6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"56d2f82ba6d641979a714fa36948aa76":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"5daef6d11c6e422db99ad239ab4d0a89":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"698ba275885e4e09bde471f044cd8bf9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fd0b584c87ff43ccb94ede84cc2d68be":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d0454bae563545fc87e77af8522dfd49":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_801406babaaf4de8a91644561f34961d","IPY_MODEL_b3bdddc8789548ff9d7d5aff2ecbb407","IPY_MODEL_c7f28e4b7dea4e79a4a9a19dd7bc5266"],"layout":"IPY_MODEL_810137fe0ea84edba1bc7d2b99263010"}},"801406babaaf4de8a91644561f34961d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_58ecc1ac083a4b35bfbf2eebac57d44e","placeholder":"​","style":"IPY_MODEL_a222ef5d456f4011af8b3d999401a379","value":"Generating test split: "}},"b3bdddc8789548ff9d7d5aff2ecbb407":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cf12cac7efe84db9b799a06336959d7c","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1b150db0e7114291bea75343fe4ef000","value":1}},"c7f28e4b7dea4e79a4a9a19dd7bc5266":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_25e3f8ff94c54445bdfa5e4ee898874f","placeholder":"​","style":"IPY_MODEL_ef374a1857ca430c86bf09dedf9bfa70","value":" 400/0 [00:00&lt;00:00, 6416.69 examples/s]"}},"810137fe0ea84edba1bc7d2b99263010":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"58ecc1ac083a4b35bfbf2eebac57d44e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a222ef5d456f4011af8b3d999401a379":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cf12cac7efe84db9b799a06336959d7c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"1b150db0e7114291bea75343fe4ef000":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"25e3f8ff94c54445bdfa5e4ee898874f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef374a1857ca430c86bf09dedf9bfa70":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"770f0910475c48ddb761491fbd2d1a6f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1daedb63a46f44848dec54c1cd1fa1ef","IPY_MODEL_c7ddc98f8ed24757b759e535a2e7be32","IPY_MODEL_408bf721d0a841f194ac140fc0011820"],"layout":"IPY_MODEL_9c7f35f641d549bb885f43c981635eb7"}},"1daedb63a46f44848dec54c1cd1fa1ef":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f4c24864a8334ff9938cca4d72de6951","placeholder":"​","style":"IPY_MODEL_098fb1fa92404f6e8d9b86bba36bff4b","value":"Map: 100%"}},"c7ddc98f8ed24757b759e535a2e7be32":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7ccf43ed79c04b4e8daca192207d0126","max":26942,"min":0,"orientation":"horizontal","style":"IPY_MODEL_85799eb3f579487894ba777813aa063d","value":26942}},"408bf721d0a841f194ac140fc0011820":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_311c108c8e6d46719cacda46ae25901b","placeholder":"​","style":"IPY_MODEL_5e2ca5f227d74f6a8e7486a233fe7c9d","value":" 26942/26942 [00:03&lt;00:00, 10652.39 examples/s]"}},"9c7f35f641d549bb885f43c981635eb7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f4c24864a8334ff9938cca4d72de6951":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"098fb1fa92404f6e8d9b86bba36bff4b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7ccf43ed79c04b4e8daca192207d0126":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"85799eb3f579487894ba777813aa063d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"311c108c8e6d46719cacda46ae25901b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5e2ca5f227d74f6a8e7486a233fe7c9d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bff9d7f6ba93401a97d51702e6c258f9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_daa732720a314b1882722a36c68371f7","IPY_MODEL_027b16ee040c446a8d32bb3f9c48ec1c","IPY_MODEL_0fd40c06ba1f4718b725570e3a558cf0"],"layout":"IPY_MODEL_d405f6adbb654db39bc268ec404498d6"}},"daa732720a314b1882722a36c68371f7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2eeab57bd0f5414481d631bde80326f5","placeholder":"​","style":"IPY_MODEL_14bf3a1e10954c64a726f4c0013cbece","value":"Map: 100%"}},"027b16ee040c446a8d32bb3f9c48ec1c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9e4308e982a5460ab3cc1f0dbc2e915c","max":1497,"min":0,"orientation":"horizontal","style":"IPY_MODEL_166f31f9cc3944ce9659611414987e49","value":1497}},"0fd40c06ba1f4718b725570e3a558cf0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3255a135111547e9ba76470cf2a91697","placeholder":"​","style":"IPY_MODEL_02e6b0919cf84656928cd09837d5505c","value":" 1497/1497 [00:00&lt;00:00, 9197.59 examples/s]"}},"d405f6adbb654db39bc268ec404498d6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2eeab57bd0f5414481d631bde80326f5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"14bf3a1e10954c64a726f4c0013cbece":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9e4308e982a5460ab3cc1f0dbc2e915c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"166f31f9cc3944ce9659611414987e49":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3255a135111547e9ba76470cf2a91697":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"02e6b0919cf84656928cd09837d5505c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"209966e32d354f7bbf5ea7005eca81d3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_354e10d60bec4b50b8b9a14a0e80b65b","IPY_MODEL_436674a570984df5a79b83595a5662e0","IPY_MODEL_9ae43e89c28f48f6bbd04a4008b71602"],"layout":"IPY_MODEL_585c0cde639f48abb5eb1865056b7dfa"}},"354e10d60bec4b50b8b9a14a0e80b65b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_313772ac55a4406a9e39a5e5b283e783","placeholder":"​","style":"IPY_MODEL_5989681f85414248ad8d870290bd49e9","value":"Map: 100%"}},"436674a570984df5a79b83595a5662e0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e5d2e82dff3f403d97958d385725a3c7","max":400,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e32e59db64cf43d0b486e230651c2e33","value":400}},"9ae43e89c28f48f6bbd04a4008b71602":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8a5cca531866491b867cbb48833a5846","placeholder":"​","style":"IPY_MODEL_89165379d9ec49c99941bac9ede813d4","value":" 400/400 [00:00&lt;00:00, 10115.59 examples/s]"}},"585c0cde639f48abb5eb1865056b7dfa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"313772ac55a4406a9e39a5e5b283e783":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5989681f85414248ad8d870290bd49e9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e5d2e82dff3f403d97958d385725a3c7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e32e59db64cf43d0b486e230651c2e33":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8a5cca531866491b867cbb48833a5846":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"89165379d9ec49c99941bac9ede813d4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}